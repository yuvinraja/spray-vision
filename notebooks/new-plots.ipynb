{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30567da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1. Import Required Libraries\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- Filter warnings for a cleaner output ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f707c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2. Configuration Block\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# --- Define file paths ---\n",
    "# Note: Update this base path to your project's root directory\n",
    "BASE_PATH = r\"d:\\GitHub Repos\\spray-vision\"\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data\", \"processed\", \"preprocessed_dataset.csv\")\n",
    "MODELS_DIR = os.path.join(BASE_PATH, \"models\")\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"outputs\")\n",
    "\n",
    "# --- Define model and feature names ---\n",
    "INPUTS = [\"time\", \"chamb_pressure\", \"cham_temp\", \"injection_pres\", \"density\", \"viscosity\"]\n",
    "TARGETS = [\"angle_mie\", \"length_mie\", \"angle_shadow\", \"length_shadow\"]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Plotting Style ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 15)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3. Data Loading and Preparation Functions\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def load_and_prepare_data(data_path, inputs, targets, test_size=0.2, random_state=42):\n",
    "    \"\"\"Loads, renames, and splits the dataset.\"\"\"\n",
    "    try:\n",
    "        raw_df = pd.read_csv(data_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {data_path}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    rename_map = {\n",
    "        \"Time_ms\": \"time\", \"Pc_bar\": \"chamb_pressure\", \"Tc_K\": \"cham_temp\",\n",
    "        \"Pinj_bar\": \"injection_pres\", \"rho_kgm3\": \"density\", \"mu_Pas\": \"viscosity\",\n",
    "        \"angle_shadow_deg\": \"angle_shadow\", \"len_shadow_L_D\": \"length_shadow\",\n",
    "        \"angle_mie_deg\": \"angle_mie\", \"len_mie_L_D\": \"length_mie\",\n",
    "    }\n",
    "    df = raw_df.rename(columns=rename_map)\n",
    "    \n",
    "    X = df[inputs]\n",
    "    y = df[targets]\n",
    "    runs = df[\"run\"] # for stratified split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=runs\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Data loaded and split successfully.\")\n",
    "    print(f\"  Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"  Test set shape:     X={X_test.shape}, y={y_test.shape}\")\n",
    "    \n",
    "    return df, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc367ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4. Model Loading Functions - Scikit-learn Models\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def load_sklearn_models(models_dir):\n",
    "    \"\"\"Loads all scikit-learn models from .joblib files.\"\"\"\n",
    "    baseline_models = {\n",
    "        'Linear Regression': 'LinearRegression_regressor.joblib',\n",
    "        'Decision Tree': 'DecisionTree_regressor.joblib',\n",
    "        'Random Forest': 'RandomForest_regressor.joblib',\n",
    "        'Gradient Boosting': 'GradientBoosting_regressor.joblib',\n",
    "        'SVR': 'SVR_regressor.joblib',\n",
    "        'KNN': 'KNN_regressor.joblib'\n",
    "    }\n",
    "    loaded_models = {}\n",
    "    print(\"\\nLoading scikit-learn models...\")\n",
    "    for model_name, model_file in baseline_models.items():\n",
    "        try:\n",
    "            model_path = os.path.join(models_dir, model_file)\n",
    "            loaded_models[model_name] = joblib.load(model_path)\n",
    "            print(f\"  ✓ {model_name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ✗ {model_name}: Model file not found.\")\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56849e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4. Model Loading Functions - ANN Model\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def load_ann_model(models_dir):\n",
    "    \"\"\"Loads the Keras ANN model, handling version compatibility issues.\"\"\"\n",
    "    print(\"\\nLoading ANN model...\")\n",
    "    models_to_try = [\n",
    "        ('ANN_improved_regressor.h5', 'ANN (Improved)'),\n",
    "        ('ANN_regressor.h5', 'ANN')\n",
    "    ]\n",
    "    for model_file, model_name in models_to_try:\n",
    "        try:\n",
    "            model_path = os.path.join(models_dir, model_file)\n",
    "            # Load model without compiling to avoid deserialization errors\n",
    "            model = keras.models.load_model(model_path, compile=False)\n",
    "            # Re-compile with a standard optimizer and loss\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            print(f\"  ✓ {model_name} loaded successfully.\")\n",
    "            \n",
    "            # Load corresponding scalers\n",
    "            input_scaler = joblib.load(os.path.join(models_dir, 'ann_input_scaler.joblib'))\n",
    "            target_scaler = joblib.load(os.path.join(models_dir, 'ann_target_scaler.joblib'))\n",
    "            print(\"  ✓ ANN scalers loaded.\")\n",
    "            \n",
    "            return {model_name: model}, input_scaler, target_scaler\n",
    "        except (IOError, FileNotFoundError):\n",
    "            print(f\"  ! {model_name} or its scalers not found. Trying next...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ An error occurred loading {model_name}: {e}\")\n",
    "            \n",
    "    print(\"  ✗ All ANN models failed to load.\")\n",
    "    return {}, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5. Prediction and Evaluation Functions - Generate Predictions\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def generate_predictions(X_test, sklearn_models, ann_model_dict, ann_input_scaler, ann_target_scaler):\n",
    "    \"\"\"Generates predictions for all loaded models.\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Scikit-learn model predictions\n",
    "    for name, model in sklearn_models.items():\n",
    "        predictions[name] = model.predict(X_test)\n",
    "        \n",
    "    # ANN model predictions\n",
    "    if ann_model_dict:\n",
    "        ann_name, ann_model = list(ann_model_dict.items())[0]\n",
    "        X_test_scaled = ann_input_scaler.transform(X_test)\n",
    "        y_pred_scaled = ann_model.predict(X_test_scaled, verbose=0)\n",
    "        predictions[ann_name] = ann_target_scaler.inverse_transform(y_pred_scaled)\n",
    "        \n",
    "    print(f\"\\n✓ Predictions generated for {len(predictions)} models.\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5. Prediction and Evaluation Functions - Evaluate Performance\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def evaluate_performance(y_true, predictions, targets):\n",
    "    \"\"\"Calculates R², MAE, and RMSE and returns a summary DataFrame.\"\"\"\n",
    "    results = []\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        for i, target in enumerate(targets):\n",
    "            r2 = r2_score(y_true[target], y_pred[:, i])\n",
    "            mae = mean_absolute_error(y_true[target], y_pred[:, i])\n",
    "            rmse = np.sqrt(mean_squared_error(y_true[target], y_pred[:, i]))\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Target': target,\n",
    "                'R²': r2,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "    \n",
    "    # Calculate overall average metrics for each model\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    overall_metrics = metrics_df.groupby('Model')[['R²', 'MAE', 'RMSE']].mean().reset_index()\n",
    "    overall_metrics['Target'] = 'Overall (Avg)'\n",
    "    \n",
    "    # Combine and sort\n",
    "    summary_df = pd.concat([metrics_df, overall_metrics]).sort_values(\n",
    "        by=['Target', 'R²'], ascending=[True, False]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6. Visualization Function\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def plot_true_vs_predicted(y_true, predictions, targets, metrics_df, output_dir):\n",
    "    \"\"\"Creates and saves a grid of True vs. Predicted plots for each target.\"\"\"\n",
    "    n_targets = len(targets)\n",
    "    n_models = len(predictions)\n",
    "    colors = sns.color_palette(\"husl\", n_models)\n",
    "    model_colors = {model: color for model, color in zip(predictions.keys(), colors)}\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, target in enumerate(targets):\n",
    "        fig, axes = plt.subplots(2, (n_models + 1) // 2, figsize=(20, 10), constrained_layout=True)\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle(f'True vs. Predicted Values for: {target.replace(\"_\", \" \").title()}', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        y_true_target = y_true[target]\n",
    "        min_val = y_true_target.min()\n",
    "        max_val = y_true_target.max()\n",
    "        \n",
    "        for j, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "            ax = axes[j]\n",
    "            y_pred_target = y_pred[:, i]\n",
    "            \n",
    "            # Update plot range\n",
    "            min_val = min(min_val, y_pred_target.min())\n",
    "            max_val = max(max_val, y_pred_target.max())\n",
    "            \n",
    "            # Plot data\n",
    "            ax.scatter(y_true_target, y_pred_target, alpha=0.6, s=50, color=model_colors[model_name], edgecolors='k', lw=0.5)\n",
    "            \n",
    "            # Metrics text\n",
    "            metrics = metrics_df[(metrics_df['Model'] == model_name) & (metrics_df['Target'] == target)].iloc[0]\n",
    "            r2, mae = metrics['R²'], metrics['MAE']\n",
    "            ax.set_title(f'{model_name}\\nR²={r2:.3f} | MAE={mae:.3f}', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel('True Values', fontsize=10)\n",
    "            ax.set_ylabel('Predicted Values', fontsize=10)\n",
    "            ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        # Finalize and save each target's plot\n",
    "        range_pad = (max_val - min_val) * 0.1\n",
    "        for ax in axes[:n_models]: # Only format used axes\n",
    "            ax.plot([min_val - range_pad, max_val + range_pad], [min_val - range_pad, max_val + range_pad], 'r--', lw=2)\n",
    "            ax.set_xlim(min_val - range_pad, max_val + range_pad)\n",
    "            ax.set_ylim(min_val - range_pad, max_val + range_pad)\n",
    "            ax.set_aspect('equal')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for k in range(n_models, len(axes)):\n",
    "            axes[k].set_visible(False)\n",
    "            \n",
    "        plot_filename = os.path.join(output_dir, f'true_vs_predicted_{target}.png')\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ✓ Plot saved to: {plot_filename}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Main Execution Block - Load Data\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# --- Load Data ---\n",
    "full_df, X_train, X_test, y_train, y_test = load_and_prepare_data(DATA_PATH, INPUTS, TARGETS, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Main Execution Block - Load Models\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# --- Load Models ---\n",
    "sklearn_models = load_sklearn_models(MODELS_DIR)\n",
    "ann_model_dict, ann_input_scaler, ann_target_scaler = load_ann_model(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Main Execution Block - Generate Predictions\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# --- Generate Predictions ---\n",
    "all_predictions = generate_predictions(\n",
    "    X_test, sklearn_models, ann_model_dict, ann_input_scaler, ann_target_scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c288cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Main Execution Block - Evaluate Performance and Display Results\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# --- Evaluate Performance ---\n",
    "if all_predictions:\n",
    "    metrics_summary_df = evaluate_performance(y_test, all_predictions, TARGETS)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY (Sorted by Overall R²)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display overall results sorted by R²\n",
    "    overall_df = metrics_summary_df[metrics_summary_df['Target'] == 'Overall (Avg)'].sort_values('R²', ascending=False)\n",
    "    print(overall_df.to_string(index=False))\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_filename = os.path.join(OUTPUT_DIR, \"detailed_model_metrics.csv\")\n",
    "    metrics_summary_df.to_csv(metrics_filename, index=False)\n",
    "    print(f\"\\n✓ Detailed metrics saved to: {metrics_filename}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No predictions were generated. Cannot evaluate performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Main Execution Block - Create and Save Visualizations\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# --- Create and Save Visualizations ---\n",
    "if all_predictions:\n",
    "    print(\"\\nGenerating and saving plots...\")\n",
    "    plot_true_vs_predicted(y_test, all_predictions, TARGETS, metrics_summary_df, OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\n✅ Analysis complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No models were loaded. Cannot generate predictions or plots.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
